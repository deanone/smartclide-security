# -*- coding: utf-8 -*-
"""
Created on Tue Jul 13 14:34:12 2021

@author: iliaskaloup
"""

import numpy as np
import pandas as pd
import os, json, time, sys, re

import torch

from transformers import Trainer
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import RobertaTokenizer, RobertaConfig, RobertaModel, RobertaForSequenceClassification

from sklearn import preprocessing

device = torch.device("cpu")
#print(device)

# define seeder
seed = 123
np.random.seed(seed)

def readFiles(inputPath, language):
    if language == "cpp":
        suffix = ".cpp"
    elif language == "java":
        suffix = ".java"
        
    paths=[]
    for root, dirs, files in os.walk(inputPath):
        for file in files:
            if (file.endswith(suffix)):
                 #print(os.path.join(root, file))
                 paths.append(root+"\\"+file)                 
    return paths

def createListOfTokens(codeFilename):
	"""
	Given a file name in the current working directory, read each line 
	and append it to a list.
	Return: the contents of the file as a list of strings
	"""
	codeLinesList = []

	with open(codeFilename, "r", encoding='utf-8') as fin:
		for line in fin:
			codeLinesList.append(line)

	return codeLinesList

def listToString(s): 
    
    # initialize an empty string
    str1 = "" 
    
    # traverse in the string  
    for ele in s: 
        str1 += ele  
    
    # return string  
    return str1 

def stringToList(string):
    codeLinesList = []
    for line in string.splitlines():
        codeLinesList.append(line)
    return codeLinesList

def remove_comments(content):
    def gen_content():
        block_comment = False
        line_comment = False
        probably_a_comment = False
        for character in content:
            if not line_comment and not block_comment and character == '/':
                probably_a_comment = True
                continue

            if block_comment and character == '*':
                probably_a_comment = True
                continue

            if line_comment and character == '\n':
                line_comment = False
                yield '\n'
            elif block_comment and probably_a_comment and character == '/':
                block_comment = False
            elif not line_comment and not block_comment:
                if probably_a_comment:
                    if character == '/':
                        line_comment = True
                    elif character == '*':
                        block_comment = True
                    else:
                        yield '/'
                        yield character
                else:
                    yield character
            probably_a_comment = False

    return ''.join(gen_content())

def dropHeaders(lines):
    linList = []
    for line in lines:
        if not re.search('#include',line):
             if not re.search('# include',line):
                  if not re.search('import',line):
                       if not re.search('package',line):
                           linList.append(line)
    return linList

def dropBlank(tokens0):
    tokens = []
    for i in range(0, len(tokens0)):
        temp = tokens0[i]
        if temp != '':
            tokens.append(temp)
    return tokens

def tokenizeLines(codeLinesList):
	codeTokens = []

	for line in codeLinesList:
		templineTokens = re.split('[\.,\[\];:(\s)?\\\\!\t{}"<>+=~*&^%/|\\-\']', line)
		codeTokens.extend(templineTokens)

	return codeTokens

def uniqueWords(data):
    allWords = []
    for i in range(len(data)):
        for j in range(len(data[i])):
            allWords.append(data[i][j])
    
    vc = pd.Series(allWords).value_counts()
    uniques=vc.index.values.tolist()        
    return allWords, uniques, vc


#main
def getVul(inputPath, language):
    milli_sec1 = int(round(time.time() * 1000))
    
    #parse all files
    print("\nTokenization\n")
    #read files
    paths = readFiles(inputPath, language)
    #paths = paths[0:100]
    
    if len(paths) == 0:
        sys.exit("There are no files to analyze")
    
    allTokens0 = []
    for i in range(0, len(paths)):
        path = paths[i]
        
        #tokenize source code in a list of lines
        lines0 = createListOfTokens(path)
        
        #convert source code from list of lines to string
        stringLines = listToString(lines0)
        
        stringLinesNoDigit = re.sub(r"$\d+\W+|\b\d+\b|\W+\d+$", "<numId$>", stringLines) #replace numbers 
        
        stringLinesNoStr = re.sub(r'(["])(?:(?=(\\?))\2.)*?\1', "<strId$>", stringLinesNoDigit) #replace strings
        
        stringLinesNoChar = re.sub(r"(['])(?:(?=(\\?))\2.)*?\1", "<strId$>", stringLinesNoStr) #replace chars
        
        #remove comments from source code
        linesNoCom = remove_comments(stringLinesNoChar)
        
        #convert source code from string to list of lines
        lines = stringToList(linesNoCom)
        
        #remove headers
        lines = dropHeaders(lines)
        
        #tokenize lines to list of words
        tokens0 = tokenizeLines(lines)
        
        #remove blank lines
        tokens = dropBlank(tokens0)
        
        #merged all files into one list
        allTokens0.append(tokens)
        
    X = []
    for i in range (0, len(allTokens0)):
        d = allTokens0[i]
        '''d.pop(0)
        d.pop(0)
        d.pop(0)'''
        s = str(d)
        s = s.replace(',', '')
        s = s.replace("'", "")
        s = s.replace('"', "")
        s = s.replace('[', "")
        s = s.replace(']', "")
        X.append(s)
    
    # Define pretrained tokenizer
    model_name = "bert-base-uncased"
    tokenizer = BertTokenizer.from_pretrained(model_name)
    
    X_tokenized = tokenizer(X, padding=True, truncation=True, max_length=512)
    
    # Create torch dataset
    class Dataset(torch.utils.data.Dataset):
        def __init__(self, encodings, labels=None):
            self.encodings = encodings
            self.labels = labels
    
        def __getitem__(self, idx):
            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
            if self.labels:
                item["labels"] = torch.tensor(self.labels[idx])
            return item
    
        def __len__(self):
            return len(self.encodings["input_ids"])
        
    dataset = Dataset(X_tokenized)
    
    print("Load model\n")
    # Load trained model        
    if language == "cpp":
        model_path = "models/model_save_cpp"
    elif language == "java":
        model_path = "models/model_save_java"
    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)
    print(model)
    
    # Define test trainer
    test_trainer = Trainer(model)
    
    # Make prediction
    raw_pred, _, _ = test_trainer.predict(dataset)
    
    # Preprocess raw predictions
    y_pred = np.argmax(raw_pred, axis=1)
    
    #print("Raw Pred: ",raw_pred)
    scores = []
    for sc in raw_pred:
        if sc[0] >= sc[1]:
            scores.append(sc[0])
        else:
            scores.append(sc[1])
    
    scaled = preprocessing.minmax_scale(scores, feature_range=(0, 1), axis=0, copy=True)
    
    for i in range(0, len(scaled)):
        if y_pred[i] == 1 and scaled[i] < 0.5:
            scaled[i] = scaled[i] + 0.5
        elif y_pred[i] == 0 and scaled[i] >= 0.5:
            scaled[i] = scaled[i] - 0.5
            
    for i in range(0, len(scaled)):
        if scaled[i] >= 0.50 and scaled[i] < 0.90:
            y_pred[i] = 0
    
    
    preds = pd.DataFrame(columns=['file_path','vulnerability_score', 'vulnerability_flag'])
    preds['file_path'] = paths
    preds['vulnerability_score'] = scaled
    preds['vulnerability_flag'] = y_pred
    
    #save preds to json
    result = preds.to_json(orient="records")
    parsed = json.loads(result)
    jsonFilePath = "output.json"
    with open(jsonFilePath, 'w') as json_file:
        json.dump(parsed, json_file, indent=4)
    
    milli_sec2 = int(round(time.time() * 1000))
    print("End of analysis after "+ str((milli_sec2-milli_sec1)/1000)+" seconds\n")
    return jsonFilePath


